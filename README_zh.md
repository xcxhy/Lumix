# Lumix

这是一个准备大语言模型数据的开源项目，由于大家都在卷模型的预训练与指令微调，大部分公开的项目也很少提及处理清洗数据的细节。大部分人应该与我一样，只能在有限的资源中训练模型，不得不将注意力从模型的规模转移至数据的规模与质量上。

我希望这个项目能够帮助到大家，尽可能完成数据的清洗工作，让大家能够更加专注于模型的训练与微调！

## News

- [ ] 聚类与质量分类
- [X] 11月，基本功能已完成！

## 项目结构

* [解析文档](./docs/zh/README_Store_zh.md "数据存储格式")
* [固定格式](./docs/zh/README_Store_zh.md "固定格式")
* [添加唯一ID](./docs/zh/README_Store_zh.md "反向确认")
* [文档分类](./docs/zh/README_CLASS_zh.md)
* [文档去重](./docs/zh/README_DP_zh.md)
* [文档过滤](./docs/zh/README_Filter_zh.md)
* [文本聚类](./docs/zh/README_CLUSTER_zh.md)
* [文本清洗](./docs/zh/README_CLEAN_zh.md)

---

## 处理流程

通过最先解析文档，处理不同来源的文档，如 `word`，`pdf`等，通过对应的算法解析出每篇文档中对应的内容，固定不同来源不同语言不同内容的文档，`添加唯一ID`，同时固定标准的存储格式。之后，我们对这些数据进行内容端分类，添加对应的类别字段。再进行去重，`Minhash`与 `Simhash`都可以进行去重，保存去重后的 `Minhash`索引与文档 `ID`，方便后续新增数据去重。之后根据去重后的 `ID`，取对应的文本数据，利用过滤模型，过滤掉存在乱码的，过短的以及语义不通的文档，在去重 `ID`的基础上再精简 `ID`进行存储；最后，我们利用保留的 `ID`，获取对应的文本，对具体的文本进行一定的清洗工作，制作对应的数据集。

## 使用方式

配置相关环境提供了两种方案：
第一，根据yml配置文件安装环境

```
conda env create -f environment.yml
conda activate <env_name>
```

第二，提供conda pack安装包供在云端下载，直接激活

```
tar -xvf fast.tar
source fast/bin/activate
```

该环境在 `CUDA Version: 11.6`以上的liunx环境中能够正常运行，如果使用windows环境有可能会存在有些包不支持的情况。

---

## 快速使用

## 数据存储

所有的数据都会存储在 `Data`目录下，`Data`下存储结构如下：

* `origin`：存储最原始的解析后的文本数据
* `unique`：存储固定格式与增添 `ID`后的文本，按文本来源进行分类存储
* `classify`：存储分类后的文本数据，与 `unique`文件夹的结构与存储数据基本一致，只增加类别标签
* `ids`：存储文档数据的的 `ID`列表，其中分为 `deduplicated `与 `filtered `两个文件夹，`deduplicated `文件夹中存储去重后的 `ID`列表与重复的 `ID`列表，`filtered `文件夹中存储的则是在去重基础上再进行文档过滤后的 `ID`列表
* `hash_index`：存储 `minhash`的索引表，分为通用与领域两个文件夹进行存储。
* `clean`：存储走完整个清洗流程后供使用的数据
* `unique_ids.json`：该文件是存储已经存在的 `ID`列表，防止新数据添加 `ID`时重复
* `filename_to_ids.json`：存储每个文本文件中的 `ID`列表

---

## 解析文档

文档的类型多种多样，为了保证其统一性，方便后续处理。首先，需要对不同的格式的文档进行读取与存储操作，如 `txt`,`json`,`docx`,`pdf`等格式。针对不同格式的文档可以使用不同的包进行读取操作：

|   数据格式   |           涉及库           |
| :-----------: | :------------------------: |
|      txt      |             -             |
| xlsx/csv/xls |           pandas           |
| docx/doc/epub |     python-docx/pandoc     |
|      pdf      | PyPDF2/pdfplumber/ocrmypdf |
|   ppt/pptx   |        unstructured        |

&emsp;&emsp;其中 `doc`可以转换至 `docx`，`xls`转换至 `xlsx`等进行读取，复杂的格式为 `PDF`，完美的读取 `PDF`格式的文本内容，能过过滤掉页眉页脚等影响文本连续性的内容，同时防止读取乱码，缺失与重复；同时，`PDF`的读取还会涉及单双栏文档的问题，根据我们测试的结果来看，使用 `paddleOCR`读取是效果最好的，但是也还是不能避免少量问题。手动工具里，`wps`有 `pdf`转 `docx`的工具，`adobe`有解析 `PDF`，提取内容的 `API`供访问.

### 存储格式

&emsp;&emsp;存储成什么格式没有什么固定的要求，也没有好坏之分，为的是方便后续的进一步处理与使用。

储存的格式一定要满足以下几点：

&emsp;&emsp;1）唯一性，是指每一个文档需要相互独立，能够通过一定的方式找到，比如构建唯一 `ID`，在查找时较为方便。

&emsp;&emsp;2）有效文本统一性，是指不同的来源的文档，在进行存储读取时，要统一格式，比如文档的主体文本，都使用 `text`字段进行保存，其他的信息都使用 `meta`字段建立新字典保存，这样虽然不同来源不同格式的文档解析出的信息不同，需要存储的信息也不同，但是我们存储读取文件的代码并不需要更新。

`example1`：

```

{
    "text": xx,
    "meta": {
        "timestamp" : xx,
        "url" : xx
        }
}

```

`example2`：

```

{
    "file_title": xx,
    "file_type": xx,
    "file_address": xx,
    "text": xx
}

```

&emsp;&emsp;以上是两种存储格式的实例，需要使用一个字典来存储一篇文档；同时，需要将关键文本使用 `text`字段存储，其他文本信息则两种方式存储都行。
